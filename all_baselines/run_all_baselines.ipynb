{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f432cb0d-394a-48f3-b8f0-63c96ce96de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/jielinq/miniconda3/envs/minigpt4/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/data/home/jielinq/miniconda3/envs/minigpt4/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from lavis.models import load_model_and_preprocess\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04f5d4a-e078-4818-bce2-e49841d5d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from pycocoevalcap.cider import cider\n",
    "from pycocoevalcap.spice import spice\n",
    "from bert_score import score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1b499bb-59b7-4aa3-885e-5c9519871f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minigpt4 import MiniGPT4\n",
    "from blip_processor import Blip2ImageEvalProcessor\n",
    "from conversation import Chat, CONV_VISION\n",
    "\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c48c65-f2ca-4f20-8393-8a18ba353ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52e1261a-c9df-46e9-b01d-7324b8bde8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5743c98-43f1-4263-8c96-da8f7e655112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_words(sentence):\n",
    "    return sentence.split(' ')\n",
    "\n",
    "def Rouge(GT_caption, generated_caption): # returns list of all rouge_n needed in (precision,recall,fmeasure) for each\n",
    "    res = []\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'], use_stemmer=True)\n",
    "    output = scorer.score(GT_caption, generated_caption)\n",
    "    for r in output:\n",
    "        score = output[r]\n",
    "        res.append((score.precision, score.recall, score.fmeasure))\n",
    "    return res[0][2], res[1][2], res[2][2]\n",
    "\n",
    "def Bleu_4(GT_caption, generated_caption):\n",
    "    GT_words = split_words(GT_caption)\n",
    "    gen_words = split_words(generated_caption)\n",
    "    weights = (1./4., 1./4., 1./4., 1./4.)\n",
    "    return corpus_bleu([[GT_words]], [gen_words], weights)\n",
    "\n",
    "def METEOR(GT_caption, generated_caption):\n",
    "    GT_words = split_words(GT_caption)\n",
    "    gen_words = split_words(generated_caption)\n",
    "    return meteor_score([GT_words], gen_words)\n",
    "\n",
    "def CIDEr(GT_caption, generated_caption):\n",
    "    scorer = cider.Cider()\n",
    "    return scorer.compute_score({0:[GT_caption]}, {0:[generated_caption]})[0]\n",
    "\n",
    "def SPICE(GT_caption, generated_caption):\n",
    "    scorer = spice.Spice()\n",
    "    return scorer.compute_score({0:[GT_caption]}, {0:[generated_caption]})[0]\n",
    "\n",
    "def BertScore(GT_caption, generated_caption):\n",
    "    P, R, F1 = score([generated_caption], [GT_caption], lang=\"en\", verbose=True)\n",
    "    #return P.tolist()[0], R.tolist()[0], F1.tolist()[0]\n",
    "    return F1.tolist()[0]\n",
    "\n",
    "def acc_full(GT_caption, generated_caption):\n",
    "    GT_caption = GT_caption.lower()\n",
    "    generated_caption = generated_caption.lower()\n",
    "    return int(GT_caption in generated_caption)\n",
    "\n",
    "def acc_part(GT_caption, generated_caption):\n",
    "    GT_words = split_words(GT_caption.lower())\n",
    "    generated_caption = generated_caption.lower()\n",
    "    \n",
    "    in_generated = 0\n",
    "    for word in GT_words:\n",
    "        if word in generated_caption:\n",
    "            in_generated += 1\n",
    "            \n",
    "    return in_generated / len(GT_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c32ee012-ae9e-46d2-b064-312da47dde58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_evaluation(GT_caption, generated_caption):\n",
    "    rouge1, rouge2, rougeL = Rouge(GT_caption, generated_caption)\n",
    "    bleu4 = Bleu_4(GT_caption, generated_caption)\n",
    "    meteor = METEOR(GT_caption, generated_caption)\n",
    "    cider= CIDEr(GT_caption, generated_caption)\n",
    "    spice = SPICE(GT_caption, generated_caption)\n",
    "    bertscore = BertScore(GT_caption, generated_caption)\n",
    "    \n",
    "    return rouge1, rouge2, rougeL, bleu4, meteor, cider, spice, bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018dc364-86ff-427a-b054-f0e1d82c5098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9b96915-810f-4e11-bd5b-460e28397cb8",
   "metadata": {},
   "source": [
    "## BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6967d3c5-15ea-405c-bc59-f690afc23332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:05<00:00,  1.18s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'eval'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blip_model, blip_vis_processors, _ = load_model_and_preprocess(\n",
    "    name=\"blip2_t5\", model_type=\"pretrain_flant5xxl\", is_eval=True, device=device\n",
    ")\n",
    "blip_vis_processors.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d7ac895-f6f9-416c-b356-88f365db4dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blip_get_result(image_path, question):\n",
    "    \n",
    "\n",
    "    raw_image = Image.open(image_path).convert('RGB')  \n",
    "    image = blip_vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n",
    "    generated_answer = blip_model.generate({\"image\": image, \"prompt\": \"%s\"%question})\n",
    "    \n",
    "    return generated_answer[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e470c-d97f-49a1-8a40-b15efdb5edb4",
   "metadata": {},
   "source": [
    "## MiniGPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b217dc8-e0c3-43be-a48c-66d5e1dc4654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VIT: vision_model_path=models/eva_vit_g.pth\n",
      "Loading VIT Done\n",
      "Loading Q-Former\n",
      "Loading Q-Former Done\n",
      "Loading LLAMA\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [04:26<00:00, 88.89s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLAMA Done\n",
      "Load BLIP2-LLM Checkpoint: models/pretrained_minigpt4.pth\n"
     ]
    }
   ],
   "source": [
    "minigpt4_model = MiniGPT4(\n",
    "    vision_model_path=\"models/eva_vit_g.pth\",\n",
    "    llama_model=\"models/vicuna13b_v0/\",\n",
    "    q_former_model=\"models/blip2_pretrained_flant5xxl.pth\",\n",
    ")\n",
    "\n",
    "ckpt_path = \"models/pretrained_minigpt4.pth\"\n",
    "\n",
    "print(\"Load BLIP2-LLM Checkpoint: {}\".format(ckpt_path))\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "minigpt4_model.load_state_dict(ckpt['model'], strict=False)\n",
    "\n",
    "torch.compile(minigpt4_model)\n",
    "\n",
    "minigpt4_vis_processor = Blip2ImageEvalProcessor()\n",
    "\n",
    "chat = Chat(minigpt4_model, minigpt4_vis_processor, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af184373-084e-465f-b895-c1f42562e521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minigpt4_get_result(image_path, question):\n",
    "    \n",
    "    chat_state = CONV_VISION.copy()\n",
    "    img_list = []\n",
    "    chat.upload_img(\"%s\"%image_path, chat_state, img_list)\n",
    "\n",
    "\n",
    "    num_beams = 1\n",
    "    temperature = 0.01\n",
    "\n",
    "    chat.ask(\"%s\"%question, chat_state)\n",
    "\n",
    "    # Callback for each word generated by the LLM\n",
    "    def callback_function(word):\n",
    "        print(word, end='', flush=True)\n",
    "\n",
    "    #print(\"Live output: \", end='', flush=True)\n",
    "\n",
    "    output_text = chat.answer_async(conv=chat_state,\n",
    "                                    img_list=img_list,\n",
    "                                    num_beams=num_beams,\n",
    "                                    temperature=temperature,\n",
    "                                    max_new_tokens=1024,\n",
    "                                    max_length=2048,\n",
    "                                    text_callback=callback_function)\n",
    "\n",
    "\n",
    "    #print(\"LLM response: {}\".format(output_text))\n",
    "    \n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac1277e-fa38-443c-9681-afd9509f9d33",
   "metadata": {},
   "source": [
    "## Open-Flamingo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91d8f24a-732c-4fc0-b32c-c3e5048f18bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "/data/home/jielinq/.cache/huggingface/modules/transformers_modules/anas-awadalla/mpt-1b-redpajama-200b/bfa38d4f431e091fe599d7b4cdb62972532f3c7c/attention.py:289: UserWarning: Using `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n",
      "Flamingo model initialized with 1046992944 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "from open_flamingo import create_model_and_transforms\n",
    "\n",
    "openflamingo_model, openflamingo_image_processor, openflamingo_tokenizer = create_model_and_transforms(\n",
    "    clip_vision_encoder_path=\"ViT-L-14\",\n",
    "    clip_vision_encoder_pretrained=\"openai\",\n",
    "    lang_encoder_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "    tokenizer_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "    cross_attn_every_n_layers=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "070ae0b8-228f-4973-b836-6d5f16b9681c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['vision_encoder.class_embedding', 'vision_encoder.positional_embedding', 'vision_encoder.proj', 'vision_encoder.conv1.weight', 'vision_encoder.ln_pre.weight', 'vision_encoder.ln_pre.bias', 'vision_encoder.transformer.resblocks.0.ln_1.weight', 'vision_encoder.transformer.resblocks.0.ln_1.bias', 'vision_encoder.transformer.resblocks.0.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.0.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.0.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.0.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.0.ln_2.weight', 'vision_encoder.transformer.resblocks.0.ln_2.bias', 'vision_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.1.ln_1.weight', 'vision_encoder.transformer.resblocks.1.ln_1.bias', 'vision_encoder.transformer.resblocks.1.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.1.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.1.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.1.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.1.ln_2.weight', 'vision_encoder.transformer.resblocks.1.ln_2.bias', 'vision_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.2.ln_1.weight', 'vision_encoder.transformer.resblocks.2.ln_1.bias', 'vision_encoder.transformer.resblocks.2.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.2.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.2.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.2.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.2.ln_2.weight', 'vision_encoder.transformer.resblocks.2.ln_2.bias', 'vision_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.3.ln_1.weight', 'vision_encoder.transformer.resblocks.3.ln_1.bias', 'vision_encoder.transformer.resblocks.3.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.3.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.3.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.3.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.3.ln_2.weight', 'vision_encoder.transformer.resblocks.3.ln_2.bias', 'vision_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.4.ln_1.weight', 'vision_encoder.transformer.resblocks.4.ln_1.bias', 'vision_encoder.transformer.resblocks.4.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.4.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.4.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.4.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.4.ln_2.weight', 'vision_encoder.transformer.resblocks.4.ln_2.bias', 'vision_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.5.ln_1.weight', 'vision_encoder.transformer.resblocks.5.ln_1.bias', 'vision_encoder.transformer.resblocks.5.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.5.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.5.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.5.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.5.ln_2.weight', 'vision_encoder.transformer.resblocks.5.ln_2.bias', 'vision_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.6.ln_1.weight', 'vision_encoder.transformer.resblocks.6.ln_1.bias', 'vision_encoder.transformer.resblocks.6.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.6.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.6.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.6.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.6.ln_2.weight', 'vision_encoder.transformer.resblocks.6.ln_2.bias', 'vision_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.7.ln_1.weight', 'vision_encoder.transformer.resblocks.7.ln_1.bias', 'vision_encoder.transformer.resblocks.7.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.7.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.7.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.7.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.7.ln_2.weight', 'vision_encoder.transformer.resblocks.7.ln_2.bias', 'vision_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.8.ln_1.weight', 'vision_encoder.transformer.resblocks.8.ln_1.bias', 'vision_encoder.transformer.resblocks.8.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.8.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.8.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.8.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.8.ln_2.weight', 'vision_encoder.transformer.resblocks.8.ln_2.bias', 'vision_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.9.ln_1.weight', 'vision_encoder.transformer.resblocks.9.ln_1.bias', 'vision_encoder.transformer.resblocks.9.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.9.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.9.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.9.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.9.ln_2.weight', 'vision_encoder.transformer.resblocks.9.ln_2.bias', 'vision_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.10.ln_1.weight', 'vision_encoder.transformer.resblocks.10.ln_1.bias', 'vision_encoder.transformer.resblocks.10.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.10.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.10.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.10.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.10.ln_2.weight', 'vision_encoder.transformer.resblocks.10.ln_2.bias', 'vision_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.11.ln_1.weight', 'vision_encoder.transformer.resblocks.11.ln_1.bias', 'vision_encoder.transformer.resblocks.11.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.11.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.11.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.11.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.11.ln_2.weight', 'vision_encoder.transformer.resblocks.11.ln_2.bias', 'vision_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.12.ln_1.weight', 'vision_encoder.transformer.resblocks.12.ln_1.bias', 'vision_encoder.transformer.resblocks.12.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.12.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.12.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.12.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.12.ln_2.weight', 'vision_encoder.transformer.resblocks.12.ln_2.bias', 'vision_encoder.transformer.resblocks.12.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.12.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.12.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.12.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.13.ln_1.weight', 'vision_encoder.transformer.resblocks.13.ln_1.bias', 'vision_encoder.transformer.resblocks.13.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.13.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.13.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.13.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.13.ln_2.weight', 'vision_encoder.transformer.resblocks.13.ln_2.bias', 'vision_encoder.transformer.resblocks.13.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.13.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.13.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.13.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.14.ln_1.weight', 'vision_encoder.transformer.resblocks.14.ln_1.bias', 'vision_encoder.transformer.resblocks.14.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.14.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.14.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.14.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.14.ln_2.weight', 'vision_encoder.transformer.resblocks.14.ln_2.bias', 'vision_encoder.transformer.resblocks.14.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.14.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.14.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.14.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.15.ln_1.weight', 'vision_encoder.transformer.resblocks.15.ln_1.bias', 'vision_encoder.transformer.resblocks.15.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.15.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.15.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.15.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.15.ln_2.weight', 'vision_encoder.transformer.resblocks.15.ln_2.bias', 'vision_encoder.transformer.resblocks.15.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.15.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.15.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.15.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.16.ln_1.weight', 'vision_encoder.transformer.resblocks.16.ln_1.bias', 'vision_encoder.transformer.resblocks.16.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.16.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.16.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.16.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.16.ln_2.weight', 'vision_encoder.transformer.resblocks.16.ln_2.bias', 'vision_encoder.transformer.resblocks.16.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.16.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.16.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.16.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.17.ln_1.weight', 'vision_encoder.transformer.resblocks.17.ln_1.bias', 'vision_encoder.transformer.resblocks.17.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.17.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.17.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.17.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.17.ln_2.weight', 'vision_encoder.transformer.resblocks.17.ln_2.bias', 'vision_encoder.transformer.resblocks.17.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.17.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.17.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.17.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.18.ln_1.weight', 'vision_encoder.transformer.resblocks.18.ln_1.bias', 'vision_encoder.transformer.resblocks.18.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.18.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.18.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.18.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.18.ln_2.weight', 'vision_encoder.transformer.resblocks.18.ln_2.bias', 'vision_encoder.transformer.resblocks.18.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.18.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.18.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.18.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.19.ln_1.weight', 'vision_encoder.transformer.resblocks.19.ln_1.bias', 'vision_encoder.transformer.resblocks.19.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.19.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.19.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.19.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.19.ln_2.weight', 'vision_encoder.transformer.resblocks.19.ln_2.bias', 'vision_encoder.transformer.resblocks.19.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.19.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.19.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.19.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.20.ln_1.weight', 'vision_encoder.transformer.resblocks.20.ln_1.bias', 'vision_encoder.transformer.resblocks.20.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.20.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.20.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.20.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.20.ln_2.weight', 'vision_encoder.transformer.resblocks.20.ln_2.bias', 'vision_encoder.transformer.resblocks.20.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.20.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.20.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.20.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.21.ln_1.weight', 'vision_encoder.transformer.resblocks.21.ln_1.bias', 'vision_encoder.transformer.resblocks.21.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.21.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.21.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.21.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.21.ln_2.weight', 'vision_encoder.transformer.resblocks.21.ln_2.bias', 'vision_encoder.transformer.resblocks.21.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.21.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.21.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.21.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.22.ln_1.weight', 'vision_encoder.transformer.resblocks.22.ln_1.bias', 'vision_encoder.transformer.resblocks.22.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.22.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.22.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.22.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.22.ln_2.weight', 'vision_encoder.transformer.resblocks.22.ln_2.bias', 'vision_encoder.transformer.resblocks.22.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.22.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.22.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.22.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.23.ln_1.weight', 'vision_encoder.transformer.resblocks.23.ln_1.bias', 'vision_encoder.transformer.resblocks.23.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.23.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.23.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.23.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.23.ln_2.weight', 'vision_encoder.transformer.resblocks.23.ln_2.bias', 'vision_encoder.transformer.resblocks.23.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.23.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.23.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.23.mlp.c_proj.bias', 'vision_encoder.ln_post.weight', 'vision_encoder.ln_post.bias', 'lang_encoder.transformer.blocks.0.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.ln_1.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.attn.q_ln.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.attn.k_ln.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.ln_2.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.mlp.mlp_up.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.mlp.mlp_down.weight', 'lang_encoder.transformer.ln_f.weight', 'lang_encoder.old_decoder_blocks.0.ln_1.weight', 'lang_encoder.old_decoder_blocks.0.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.0.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.0.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.0.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.0.ln_2.weight', 'lang_encoder.old_decoder_blocks.0.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.0.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.1.ln_1.weight', 'lang_encoder.old_decoder_blocks.1.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.1.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.1.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.1.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.1.ln_2.weight', 'lang_encoder.old_decoder_blocks.1.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.1.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.2.ln_1.weight', 'lang_encoder.old_decoder_blocks.2.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.2.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.2.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.2.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.2.ln_2.weight', 'lang_encoder.old_decoder_blocks.2.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.2.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.3.ln_1.weight', 'lang_encoder.old_decoder_blocks.3.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.3.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.3.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.3.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.3.ln_2.weight', 'lang_encoder.old_decoder_blocks.3.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.3.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.4.ln_1.weight', 'lang_encoder.old_decoder_blocks.4.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.4.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.4.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.4.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.4.ln_2.weight', 'lang_encoder.old_decoder_blocks.4.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.4.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.5.ln_1.weight', 'lang_encoder.old_decoder_blocks.5.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.5.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.5.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.5.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.5.ln_2.weight', 'lang_encoder.old_decoder_blocks.5.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.5.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.6.ln_1.weight', 'lang_encoder.old_decoder_blocks.6.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.6.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.6.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.6.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.6.ln_2.weight', 'lang_encoder.old_decoder_blocks.6.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.6.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.7.ln_1.weight', 'lang_encoder.old_decoder_blocks.7.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.7.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.7.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.7.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.7.ln_2.weight', 'lang_encoder.old_decoder_blocks.7.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.7.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.8.ln_1.weight', 'lang_encoder.old_decoder_blocks.8.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.8.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.8.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.8.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.8.ln_2.weight', 'lang_encoder.old_decoder_blocks.8.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.8.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.9.ln_1.weight', 'lang_encoder.old_decoder_blocks.9.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.9.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.9.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.9.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.9.ln_2.weight', 'lang_encoder.old_decoder_blocks.9.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.9.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.10.ln_1.weight', 'lang_encoder.old_decoder_blocks.10.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.10.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.10.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.10.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.10.ln_2.weight', 'lang_encoder.old_decoder_blocks.10.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.10.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.11.ln_1.weight', 'lang_encoder.old_decoder_blocks.11.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.11.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.11.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.11.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.11.ln_2.weight', 'lang_encoder.old_decoder_blocks.11.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.11.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.12.ln_1.weight', 'lang_encoder.old_decoder_blocks.12.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.12.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.12.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.12.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.12.ln_2.weight', 'lang_encoder.old_decoder_blocks.12.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.12.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.13.ln_1.weight', 'lang_encoder.old_decoder_blocks.13.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.13.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.13.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.13.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.13.ln_2.weight', 'lang_encoder.old_decoder_blocks.13.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.13.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.14.ln_1.weight', 'lang_encoder.old_decoder_blocks.14.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.14.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.14.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.14.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.14.ln_2.weight', 'lang_encoder.old_decoder_blocks.14.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.14.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.15.ln_1.weight', 'lang_encoder.old_decoder_blocks.15.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.15.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.15.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.15.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.15.ln_2.weight', 'lang_encoder.old_decoder_blocks.15.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.15.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.16.ln_1.weight', 'lang_encoder.old_decoder_blocks.16.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.16.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.16.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.16.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.16.ln_2.weight', 'lang_encoder.old_decoder_blocks.16.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.16.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.17.ln_1.weight', 'lang_encoder.old_decoder_blocks.17.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.17.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.17.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.17.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.17.ln_2.weight', 'lang_encoder.old_decoder_blocks.17.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.17.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.18.ln_1.weight', 'lang_encoder.old_decoder_blocks.18.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.18.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.18.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.18.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.18.ln_2.weight', 'lang_encoder.old_decoder_blocks.18.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.18.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.19.ln_1.weight', 'lang_encoder.old_decoder_blocks.19.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.19.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.19.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.19.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.19.ln_2.weight', 'lang_encoder.old_decoder_blocks.19.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.19.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.20.ln_1.weight', 'lang_encoder.old_decoder_blocks.20.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.20.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.20.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.20.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.20.ln_2.weight', 'lang_encoder.old_decoder_blocks.20.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.20.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.21.ln_1.weight', 'lang_encoder.old_decoder_blocks.21.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.21.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.21.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.21.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.21.ln_2.weight', 'lang_encoder.old_decoder_blocks.21.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.21.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.22.ln_1.weight', 'lang_encoder.old_decoder_blocks.22.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.22.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.22.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.22.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.22.ln_2.weight', 'lang_encoder.old_decoder_blocks.22.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.22.mlp.mlp_down.weight', 'lang_encoder.old_decoder_blocks.23.ln_1.weight', 'lang_encoder.old_decoder_blocks.23.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.23.attn.q_ln.weight', 'lang_encoder.old_decoder_blocks.23.attn.k_ln.weight', 'lang_encoder.old_decoder_blocks.23.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.23.ln_2.weight', 'lang_encoder.old_decoder_blocks.23.mlp.mlp_up.weight', 'lang_encoder.old_decoder_blocks.23.mlp.mlp_down.weight', 'lang_encoder.gated_cross_attn_layers.0.attn_gate', 'lang_encoder.gated_cross_attn_layers.0.ff_gate', 'lang_encoder.gated_cross_attn_layers.0.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.0.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.0.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.0.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.0.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.0.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.0.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.0.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.0.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.1.attn_gate', 'lang_encoder.gated_cross_attn_layers.1.ff_gate', 'lang_encoder.gated_cross_attn_layers.1.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.1.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.1.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.1.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.1.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.1.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.1.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.1.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.1.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.2.attn_gate', 'lang_encoder.gated_cross_attn_layers.2.ff_gate', 'lang_encoder.gated_cross_attn_layers.2.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.2.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.2.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.2.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.2.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.2.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.2.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.2.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.2.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.3.attn_gate', 'lang_encoder.gated_cross_attn_layers.3.ff_gate', 'lang_encoder.gated_cross_attn_layers.3.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.3.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.3.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.3.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.3.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.3.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.3.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.3.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.3.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.4.attn_gate', 'lang_encoder.gated_cross_attn_layers.4.ff_gate', 'lang_encoder.gated_cross_attn_layers.4.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.4.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.4.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.4.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.4.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.4.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.4.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.4.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.4.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.5.attn_gate', 'lang_encoder.gated_cross_attn_layers.5.ff_gate', 'lang_encoder.gated_cross_attn_layers.5.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.5.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.5.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.5.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.5.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.5.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.5.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.5.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.5.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.6.attn_gate', 'lang_encoder.gated_cross_attn_layers.6.ff_gate', 'lang_encoder.gated_cross_attn_layers.6.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.6.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.6.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.6.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.6.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.6.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.6.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.6.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.6.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.7.attn_gate', 'lang_encoder.gated_cross_attn_layers.7.ff_gate', 'lang_encoder.gated_cross_attn_layers.7.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.7.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.7.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.7.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.7.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.7.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.7.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.7.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.7.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.8.attn_gate', 'lang_encoder.gated_cross_attn_layers.8.ff_gate', 'lang_encoder.gated_cross_attn_layers.8.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.8.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.8.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.8.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.8.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.8.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.8.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.8.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.8.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.9.attn_gate', 'lang_encoder.gated_cross_attn_layers.9.ff_gate', 'lang_encoder.gated_cross_attn_layers.9.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.9.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.9.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.9.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.9.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.9.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.9.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.9.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.9.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.10.attn_gate', 'lang_encoder.gated_cross_attn_layers.10.ff_gate', 'lang_encoder.gated_cross_attn_layers.10.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.10.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.10.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.10.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.10.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.10.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.10.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.10.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.10.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.11.attn_gate', 'lang_encoder.gated_cross_attn_layers.11.ff_gate', 'lang_encoder.gated_cross_attn_layers.11.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.11.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.11.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.11.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.11.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.11.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.11.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.11.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.11.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.12.attn_gate', 'lang_encoder.gated_cross_attn_layers.12.ff_gate', 'lang_encoder.gated_cross_attn_layers.12.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.12.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.12.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.12.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.12.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.12.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.12.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.12.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.12.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.13.attn_gate', 'lang_encoder.gated_cross_attn_layers.13.ff_gate', 'lang_encoder.gated_cross_attn_layers.13.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.13.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.13.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.13.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.13.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.13.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.13.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.13.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.13.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.14.attn_gate', 'lang_encoder.gated_cross_attn_layers.14.ff_gate', 'lang_encoder.gated_cross_attn_layers.14.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.14.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.14.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.14.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.14.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.14.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.14.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.14.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.14.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.15.attn_gate', 'lang_encoder.gated_cross_attn_layers.15.ff_gate', 'lang_encoder.gated_cross_attn_layers.15.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.15.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.15.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.15.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.15.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.15.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.15.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.15.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.15.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.16.attn_gate', 'lang_encoder.gated_cross_attn_layers.16.ff_gate', 'lang_encoder.gated_cross_attn_layers.16.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.16.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.16.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.16.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.16.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.16.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.16.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.16.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.16.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.17.attn_gate', 'lang_encoder.gated_cross_attn_layers.17.ff_gate', 'lang_encoder.gated_cross_attn_layers.17.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.17.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.17.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.17.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.17.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.17.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.17.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.17.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.17.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.18.attn_gate', 'lang_encoder.gated_cross_attn_layers.18.ff_gate', 'lang_encoder.gated_cross_attn_layers.18.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.18.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.18.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.18.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.18.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.18.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.18.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.18.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.18.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.19.attn_gate', 'lang_encoder.gated_cross_attn_layers.19.ff_gate', 'lang_encoder.gated_cross_attn_layers.19.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.19.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.19.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.19.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.19.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.19.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.19.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.19.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.19.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.20.attn_gate', 'lang_encoder.gated_cross_attn_layers.20.ff_gate', 'lang_encoder.gated_cross_attn_layers.20.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.20.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.20.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.20.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.20.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.20.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.20.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.20.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.20.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.21.attn_gate', 'lang_encoder.gated_cross_attn_layers.21.ff_gate', 'lang_encoder.gated_cross_attn_layers.21.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.21.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.21.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.21.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.21.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.21.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.21.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.21.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.21.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.22.attn_gate', 'lang_encoder.gated_cross_attn_layers.22.ff_gate', 'lang_encoder.gated_cross_attn_layers.22.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.22.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.22.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.22.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.22.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.22.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.22.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.22.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.22.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.23.attn_gate', 'lang_encoder.gated_cross_attn_layers.23.ff_gate', 'lang_encoder.gated_cross_attn_layers.23.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.23.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.23.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.23.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.23.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.23.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.23.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.23.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.23.ff.3.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-3B-vitl-mpt1b\", \"checkpoint.pt\")\n",
    "openflamingo_model.load_state_dict(torch.load(checkpoint_path), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2242223-fc04-44a9-9e1a-65fe1396c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openflamingo_get_result(image_path, question):\n",
    "    \n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    Step 1: Load images\n",
    "    \"\"\"\n",
    "    demo_image_one = Image.open(\n",
    "        requests.get(\n",
    "            \"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True\n",
    "        ).raw\n",
    "    )\n",
    "\n",
    "    demo_image_two = Image.open(\n",
    "        requests.get(\n",
    "            \"http://images.cocodataset.org/test-stuff2017/000000028137.jpg\",\n",
    "            stream=True\n",
    "        ).raw\n",
    "    )\n",
    "\n",
    "    query_image = Image.open(image_path)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Step 2: Preprocessing images\n",
    "    Details: For OpenFlamingo, we expect the image to be a torch tensor of shape \n",
    "     batch_size x num_media x num_frames x channels x height x width. \n",
    "     In this case batch_size = 1, num_media = 3, num_frames = 1,\n",
    "     channels = 3, height = 224, width = 224.\n",
    "    \"\"\"\n",
    "    vision_x = [openflamingo_image_processor(demo_image_one).unsqueeze(0), openflamingo_image_processor(demo_image_two).unsqueeze(0), openflamingo_image_processor(query_image).unsqueeze(0)]\n",
    "    vision_x = torch.cat(vision_x, dim=0)\n",
    "    vision_x = vision_x.unsqueeze(1).unsqueeze(0)\n",
    "\n",
    "    \"\"\"\n",
    "    Step 3: Preprocessing text\n",
    "    Details: In the text we expect an <image> special token to indicate where an image is.\n",
    "     We also expect an <|endofchunk|> special token to indicate the end of the text \n",
    "     portion associated with an image.\n",
    "    \"\"\"\n",
    "    openflamingo_tokenizer.padding_side = \"left\" # For generation padding tokens should be on the left\n",
    "    lang_x = openflamingo_tokenizer(\n",
    "        [\"<image>An image of two cats.<|endofchunk|><image>An image of a bathroom sink.<|endofchunk|><image>%s\"%question],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Step 4: Generate text\n",
    "    \"\"\"\n",
    "    generated_text = openflamingo_model.generate(\n",
    "        vision_x=vision_x,\n",
    "        lang_x=lang_x[\"input_ids\"],\n",
    "        attention_mask=lang_x[\"attention_mask\"],\n",
    "        max_new_tokens=20,\n",
    "        num_beams=3,\n",
    "    )\n",
    "\n",
    "    #print(\"Generated text: \", tokenizer.decode(generated_text[0]))\n",
    "    \n",
    "    answer = openflamingo_tokenizer.decode(generated_text[0])[98:-15]\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2626a52e-dd04-4d93-ba9d-5440e4892579",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tsv = pd.read_csv(\"landmark_all.tsv\", sep='\\t', encoding='utf-8')\n",
    "\n",
    "df_results = pd.DataFrame(columns=['GT_answer', 'blip_generated_answer','minigpt4_generated_answer', 'openflamingo_generated_answer', \\\n",
    "                                   'rouge1_blip', 'rouge1_minigpt4', 'rouge1_openflamingo', \\\n",
    "                                   'rouge2_blip', 'rouge2_minigpt4', 'rouge2_openflamingo', \\\n",
    "                                   'rougeL_blip', 'rougeL_minigpt4', 'rougeL_openflamingo', \\\n",
    "                                   'bleu4_blip', 'bleu4_minigpt4', 'bleu4_openflamingo', \\\n",
    "                                   'meteor_blip', 'meteor_minigpt4', 'meteor_openflamingo', \\\n",
    "                                   'cider_blip', 'cider_minigpt4', 'cider_openflamingo', \\\n",
    "                                   'spice_blip', 'spice_minigpt4', 'spice_openflamingo', \\\n",
    "                                   'bertscore_blip', 'bertscore_minigpt4', 'bertscore_openflamingo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b665cf6-6bbd-4078-a5c6-9bad23e847b8",
   "metadata": {},
   "source": [
    "## Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec535a-bc2b-491f-be3e-d2963f3ca06b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data/home/jielinq/miniconda3/envs/minigpt4/lib/python3.9/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 759.9 ms\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 61.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 530.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.02 seconds, 45.90 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>The architectural style of the building in the image is Gothic. The building has a tall, pointed roof, large arched windows, and intricate carvings on the facade. The style is characterized by the use of pointed arches, ribbed vaults, and large windows.###"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data/home/jielinq/miniconda3/envs/minigpt4/lib/python3.9/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 915.4 ms\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 56.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 624.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.02 seconds, 42.34 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/data/home/jielinq/miniconda3/envs/minigpt4/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data/home/jielinq/miniconda3/envs/minigpt4/lib/python3.9/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 819.1 ms\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 58.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 568.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.02 seconds, 43.82 sentences/sec\n",
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_1830098/3887460049.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_results = df_results.append({'GT_answer':GT_answer, \\\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data/home/jielinq/miniconda3/envs/minigpt4/lib/python3.9/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 802.5 ms\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 60.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 618.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.02 seconds, 45.50 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>The architectural style of this image is Gothic Revival. The image shows a large, ornate altar with intricate carvings and a stained glass window behind it. The altar is made of stone and has a wooden top. The stained glass window is a beautiful, colorful depiction of a religious scene. The walls and floor are made of stone, and the room is dimly lit by candles.###"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data/home/jielinq/miniconda3/envs/minigpt4/lib/python3.9/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 757.9 ms\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 55.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 89.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.03 seconds, 30.34 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/data/home/jielinq/miniconda3/envs/minigpt4/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data/home/jielinq/miniconda3/envs/minigpt4/lib/python3.9/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 762.1 ms\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,101,50):\n",
    "    \n",
    "    print(i)\n",
    "    GT_answer = old_tsv.loc[i,'answer']\n",
    "    \n",
    "    ##BLIP\n",
    "    blip_generated_answer = blip_get_result(old_tsv.loc[i,'image_path'], old_tsv.loc[i,'question'])\n",
    "    \n",
    "    blip_rouge1_res, blip_rouge2_res, blip_rougeL_res, blip_bleu4_res, blip_meteor_res, blip_cider_res, blip_spice_res, blip_bertscore_res = answer_evaluation(GT_answer, blip_generated_answer)\n",
    "    \n",
    "    ##minigpt4\n",
    "    minigpt4_generated_answer = minigpt4_get_result(old_tsv.loc[i,'image_path'], old_tsv.loc[i,'question'])\n",
    "    \n",
    "    minigpt4_rouge1_res, minigpt4_rouge2_res, minigpt4_rougeL_res, minigpt4_bleu4_res, minigpt4_meteor_res, minigpt4_cider_res, minigpt4_spice_res, minigpt4_bertscore_res = answer_evaluation(GT_answer, minigpt4_generated_answer)\n",
    "    \n",
    "    ##open-flamingo\n",
    "    openflamingo_generated_answer = openflamingo_get_result(old_tsv.loc[i,'image_path'], old_tsv.loc[i,'question'])\n",
    "    \n",
    "    openflamingo_rouge1_res, openflamingo_rouge2_res, openflamingo_rougeL_res, openflamingo_bleu4_res, openflamingo_meteor_res, openflamingo_cider_res, openflamingo_spice_res, openflamingo_bertscore_res = answer_evaluation(GT_answer, openflamingo_generated_answer)\n",
    "    \n",
    "    \n",
    "    df_results = df_results.append({'GT_answer':GT_answer, \\\n",
    "                                    'blip_generated_answer': blip_generated_answer, 'minigpt4_generated_answer': minigpt4_generated_answer, 'openflamingo_generated_answer': openflamingo_generated_answer,\\\n",
    "                                    'rouge1_blip':blip_rouge1_res, 'rouge1_minigpt4':minigpt4_rouge1_res, 'rouge1_openflamingo':openflamingo_rouge1_res, \\\n",
    "                                    'rouge2_blip':blip_rouge2_res, 'rouge2_minigpt4':minigpt4_rouge2_res, 'rouge2_openflamingo':openflamingo_rouge2_res, \\\n",
    "                                    'rougeL_blip':blip_rougeL_res, 'rougeL_minigpt4':minigpt4_rougeL_res, 'rougeL_openflamingo':openflamingo_rougeL_res, \\\n",
    "                                    'bleu4_blip':blip_bleu4_res, 'bleu4_minigpt4':minigpt4_bleu4_res, 'bleu4_openflamingo':openflamingo_bleu4_res, \\\n",
    "                                    'meteor_blip':blip_meteor_res, 'meteor_minigpt4':minigpt4_meteor_res, 'meteor_openflamingo':openflamingo_meteor_res, \\\n",
    "                                    'cider_blip':blip_cider_res, 'cider_minigpt4':minigpt4_cider_res, 'cider_openflamingo':openflamingo_cider_res, \\\n",
    "                                    'spice_blip':blip_spice_res, 'spice_minigpt4':minigpt4_spice_res, 'spice_openflamingo':openflamingo_spice_res, \\\n",
    "                                    'bertscore_blip':blip_bertscore_res, 'bertscore_minigpt4':minigpt4_bertscore_res, 'bertscore_openflamingo':openflamingo_bertscore_res}, ignore_index=True)\n",
    "    \n",
    "df_results.to_csv('all_baselines_landmark.tsv', sep='\\t', encoding='utf-8', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c3a07a-3f9b-429a-9b42-bf71cccc233a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigpt4",
   "language": "python",
   "name": "minigpt4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
