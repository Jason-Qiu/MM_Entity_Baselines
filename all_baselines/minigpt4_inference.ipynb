{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1686b67b-2fe3-4cf0-9779-2dfa3af33d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/jielinq/miniconda3/envs/minigpt4/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data/home/jielinq/miniconda3/envs/minigpt4/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from minigpt4 import MiniGPT4\n",
    "from blip_processor import Blip2ImageEvalProcessor\n",
    "from conversation import Chat, CONV_VISION\n",
    "\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fcf1221-8373-468f-b146-369350db431f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VIT: vision_model_path=models/eva_vit_g.pth\n",
      "Loading VIT Done\n",
      "Loading Q-Former\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565, and set the legacy attribute accordingly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Q-Former Done\n",
      "Loading LLAMA\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLAMA Done\n",
      "Load BLIP2-LLM Checkpoint: models/pretrained_minigpt4.pth\n",
      "Models loaded in 49.457420110702515 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "model = MiniGPT4(\n",
    "    vision_model_path=\"models/eva_vit_g.pth\",\n",
    "    llama_model=\"models/vicuna13b_v0/\",\n",
    "    q_former_model=\"models/blip2_pretrained_flant5xxl.pth\",\n",
    ")\n",
    "\n",
    "ckpt_path = \"models/pretrained_minigpt4.pth\"\n",
    "\n",
    "print(\"Load BLIP2-LLM Checkpoint: {}\".format(ckpt_path))\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt['model'], strict=False)\n",
    "\n",
    "torch.compile(model)\n",
    "\n",
    "vis_processor = Blip2ImageEvalProcessor()\n",
    "\n",
    "chat = Chat(model, vis_processor, device='cuda:0')\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"Models loaded in {} seconds\".format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55bc8c92-d1b8-47c3-ab3a-47763fd08a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loaded in 0.09377050399780273 seconds\n",
      "<s>I see a large, white missile on the road. It appears to be made of metal and has a pointed nose and tail. It is sitting on the ground, leaning against a road sign. The sign says \"Danger: Missile Ahead\". There is a cloudy sky in the background.###\n",
      "LLM response: I see a large, white missile on the road. It appears to be made of metal and has a pointed nose and tail. It is sitting on the ground, leaning against a road sign. The sign says \"Danger: Missile Ahead\". There is a cloudy sky in the background.\n",
      "Conversation(system='Human provides a photo and asks questions.  Assistant answers the questions honestly and simply.', roles=('Human', 'Assistant'), messages=[['Human', '<Img><ImageHere></Img> Tell me what you see on the road.'], ['Assistant', 'I see a large, white missile on the road. It appears to be made of metal and has a pointed nose and tail. It is sitting on the ground, leaning against a road sign. The sign says \"Danger: Missile Ahead\". There is a cloudy sky in the background.']], offset=2, sep_style=<SeparatorStyle.SINGLE: 1>, sep='###', sep2=None, skip_next=False, conv_id=None)\n",
      "Generated LLM response in 12.66400408744812 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "chat_state = CONV_VISION.copy()\n",
    "img_list = []\n",
    "chat.upload_img(\"icbm_bicycle.png\", chat_state, img_list)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"Image loaded in {} seconds\".format(t1-t0))\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "num_beams = 1\n",
    "temperature = 0.01\n",
    "\n",
    "chat.ask(\"Tell me what you see on the road.\", chat_state)\n",
    "\n",
    "# Callback for each word generated by the LLM\n",
    "def callback_function(word):\n",
    "    print(word, end='', flush=True)\n",
    "\n",
    "#print(\"Live output: \", end='', flush=True)\n",
    "\n",
    "output_text = chat.answer_async(conv=chat_state,\n",
    "                                img_list=img_list,\n",
    "                                num_beams=num_beams,\n",
    "                                temperature=temperature,\n",
    "                                max_new_tokens=1024,\n",
    "                                max_length=2048,\n",
    "                                text_callback=callback_function)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"LLM response: {}\".format(output_text))\n",
    "print(chat_state)\n",
    "print(\"Generated LLM response in {} seconds\".format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e769f5c7-58f2-4296-9b05-b11a6970065c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I see a large, white missile on the road. It appears to be made of metal and has a pointed nose and tail. It is sitting on the ground, leaning against a road sign. The sign says \"Danger: Missile Ahead\". There is a cloudy sky in the background.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b1308-16c9-4048-8e92-b5262b42bcb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigpt4",
   "language": "python",
   "name": "minigpt4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
